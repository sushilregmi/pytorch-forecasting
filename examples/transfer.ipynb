{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0962125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import QuantileLoss, SMAPE, MAE, RMSE, MAPE\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from torch.utils.data import DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6206350",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set CUDA device if needed\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49bc7ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/timexer_lambda.csv\", parse_dates=[\"date\"])\n",
    "df = df.sort_values(\"date\")\n",
    "df[\"time_idx\"] = (df[\"date\"] - df[\"date\"].min()).dt.total_seconds().astype(int)\n",
    "df[\"series_id\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "281e392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split: 70% train, 10% val, 20% test\n",
    "num_train = int(len(df) * 0.7)\n",
    "num_val = int(len(df) * 0.1)\n",
    "num_test = len(df) - num_train - num_val\n",
    "test_df = df.iloc[num_train + num_val:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3092c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = 50\n",
    "batch_size = 64\n",
    "group_ids = [\"series_id\"]\n",
    "target = \"duration_sum\"\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0bfe0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_varying_known_reals = [\n",
    "    \"bytes_op0\", \"bytes_op1\", \"bytes_sum\", \"io_count\",\n",
    "    \"read_ops_count\", \"write_ops_count\",\n",
    "    \"bytes_sum_ema_short\", \"bytes_sum_ema_long\",\n",
    "    \"bytes_sum_macd\", \"bytes_sum_macd_signal\",\n",
    "]\n",
    "time_varying_unknown_reals = [\"duration_sum\"]\n",
    "static_categoricals = []\n",
    "static_reals = []\n",
    "\n",
    "covariate_scalers = {\n",
    "    var: GroupNormalizer(groups=group_ids)\n",
    "    for var in time_varying_known_reals\n",
    "}\n",
    "\n",
    "# Define horizons to evaluate\n",
    "horizons = [1, 5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97d9f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs = dict(\n",
    "    learning_rate=1e-4,  # lower LR for fine-tuning\n",
    "    hidden_size=128,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=128,\n",
    "    loss=QuantileLoss(quantiles=[0.1, 0.5, 0.9]),\n",
    "    output_size=3,\n",
    "    logging_metrics=[SMAPE(), MAE(), RMSE(), MAPE()],\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67046f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_kwargs = dict(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=0.1,\n",
    "    max_epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b63266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLossCallback(Callback):\n",
    "    def __init__(self, horizon):\n",
    "        self.horizon = horizon\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        epoch = trainer.current_epoch\n",
    "        metrics = trainer.callback_metrics\n",
    "        print(f\"[H={self.horizon}] Epoch={epoch:02d}, train_loss={metrics.get('train_loss'):.4f}, val_loss={metrics.get('val_loss'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f53e6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAndLossCallback(Callback):\n",
    "    def __init__(self, horizon):\n",
    "        self.horizon = horizon\n",
    "        self.epoch_times = []\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - self.start_time\n",
    "        self.epoch_times.append(elapsed)\n",
    "\n",
    "        epoch = trainer.current_epoch\n",
    "        metrics = trainer.callback_metrics\n",
    "        print(\n",
    "            f\"[H={self.horizon}] Epoch={epoch:02d}, \"\n",
    "            f\"train_loss={metrics.get('train_loss'):.4f}, \"\n",
    "            f\"val_loss={metrics.get('val_loss'):.4f}, \"\n",
    "            f\"time={elapsed:.2f}s\"\n",
    "        )\n",
    "\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        avg_time = np.mean(self.epoch_times)\n",
    "        print(f\"üïí [H={self.horizon}] Average epoch time: {avg_time:.2f} seconds over {len(self.epoch_times)} epochs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d277125a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fine-tuning model for horizon = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0      | train\n",
      "3  | prescalers                         | ModuleDict                      | 3.8 K  | train\n",
      "4  | encoder_variable_selection         | VariableSelectionNetwork        | 819 K  | train\n",
      "5  | decoder_variable_selection         | VariableSelectionNetwork        | 749 K  | train\n",
      "6  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K | train\n",
      "7  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K | train\n",
      "8  | lstm_encoder                       | LSTM                            | 132 K  | train\n",
      "9  | lstm_decoder                       | LSTM                            | 132 K  | train\n",
      "10 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K | train\n",
      "11 | post_lstm_add_norm_encoder         | AddNorm                         | 256    | train\n",
      "12 | multihead_attn                     | InterpretableMultiHeadAttention | 41.2 K | train\n",
      "13 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K | train\n",
      "14 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K | train\n",
      "15 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K | train\n",
      "16 | output_layer                       | Linear                          | 387    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.687     Total estimated model params size (MB)\n",
      "357       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddeb1ddfc41b4b288a138ed69a45e2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f931911b484360b0c42c10d423ee6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e6f410104f473eb7fab05d6f302782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=1] Epoch=00, train_loss=0.0004, val_loss=0.0027, time=289.89s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff42e2d249d0458cb36ebaad14818dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=1] Epoch=01, train_loss=0.0003, val_loss=0.0026, time=278.71s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31622166ca714057b8b36ccf19f6f303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=1] Epoch=02, train_loss=0.0003, val_loss=0.0025, time=317.58s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d17b25aaa14aa08eab962b752a947c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=1] Epoch=03, train_loss=0.0002, val_loss=0.0025, time=284.78s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00e380775c74e409cece7abb3288b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=1] Epoch=04, train_loss=0.0002, val_loss=0.0025, time=290.78s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d4cebd065849a8968632f690271a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=1] Epoch=05, train_loss=0.0002, val_loss=0.0025, time=276.33s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0141e277162d4fbb8eb6090feb576546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=1] Epoch=06, train_loss=0.0002, val_loss=0.0026, time=273.63s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f4c64dc58e4f17ba0266038ace2d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=1] Epoch=07, train_loss=0.0002, val_loss=0.0026, time=273.09s\n",
      "üïí [H=1] Average epoch time: 285.60 seconds over 8 epochs\n",
      "‚úÖ Fine-tuning complete for horizon 1. Saved to ckpts_lambda/horizon_1_lambda\n",
      "\n",
      "üîÅ Fine-tuning model for horizon = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0      | train\n",
      "3  | prescalers                         | ModuleDict                      | 3.8 K  | train\n",
      "4  | encoder_variable_selection         | VariableSelectionNetwork        | 819 K  | train\n",
      "5  | decoder_variable_selection         | VariableSelectionNetwork        | 749 K  | train\n",
      "6  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K | train\n",
      "7  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K | train\n",
      "8  | lstm_encoder                       | LSTM                            | 132 K  | train\n",
      "9  | lstm_decoder                       | LSTM                            | 132 K  | train\n",
      "10 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K | train\n",
      "11 | post_lstm_add_norm_encoder         | AddNorm                         | 256    | train\n",
      "12 | multihead_attn                     | InterpretableMultiHeadAttention | 41.2 K | train\n",
      "13 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K | train\n",
      "14 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K | train\n",
      "15 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K | train\n",
      "16 | output_layer                       | Linear                          | 387    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.687     Total estimated model params size (MB)\n",
      "357       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3144a4a5584553b736f30ae612c9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "764f9f1c5c6040889f3d1fbba3c29f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb4d4f2fb194016a167c03735eae812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=5] Epoch=00, train_loss=0.0004, val_loss=0.0027, time=268.83s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961c556eb5544dff889fbe470df260a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=5] Epoch=01, train_loss=0.0003, val_loss=0.0026, time=277.03s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "800783eb01a546599507aa10989b240f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=5] Epoch=02, train_loss=0.0003, val_loss=0.0026, time=276.74s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3f70c69774462982ae1f6d908fd14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=5] Epoch=03, train_loss=0.0002, val_loss=0.0026, time=277.63s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17d7eab0a94449db8d4c9a76292ce69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=5] Epoch=04, train_loss=0.0002, val_loss=0.0026, time=280.83s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de07c9059e38443ca7fd454121fad477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=5] Epoch=05, train_loss=0.0002, val_loss=0.0026, time=271.40s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03310e60f44743acbed4752fb4d53f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=5] Epoch=06, train_loss=0.0002, val_loss=0.0026, time=281.23s\n",
      "üïí [H=5] Average epoch time: 276.24 seconds over 7 epochs\n",
      "‚úÖ Fine-tuning complete for horizon 5. Saved to ckpts_lambda/horizon_5_lambda\n",
      "\n",
      "üîÅ Fine-tuning model for horizon = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0      | train\n",
      "3  | prescalers                         | ModuleDict                      | 3.8 K  | train\n",
      "4  | encoder_variable_selection         | VariableSelectionNetwork        | 819 K  | train\n",
      "5  | decoder_variable_selection         | VariableSelectionNetwork        | 749 K  | train\n",
      "6  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K | train\n",
      "7  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K | train\n",
      "8  | lstm_encoder                       | LSTM                            | 132 K  | train\n",
      "9  | lstm_decoder                       | LSTM                            | 132 K  | train\n",
      "10 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K | train\n",
      "11 | post_lstm_add_norm_encoder         | AddNorm                         | 256    | train\n",
      "12 | multihead_attn                     | InterpretableMultiHeadAttention | 41.2 K | train\n",
      "13 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K | train\n",
      "14 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K | train\n",
      "15 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K | train\n",
      "16 | output_layer                       | Linear                          | 387    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.687     Total estimated model params size (MB)\n",
      "357       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2898b8e41f40b8b517ede3c028efc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca88c21c9bb84b17938d228b70f4258c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1daa06839ae4685ab33dae4f663d9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=10] Epoch=00, train_loss=0.0004, val_loss=0.0029, time=278.92s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc10915390a4420a760b6ed59fd28f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=10] Epoch=01, train_loss=0.0003, val_loss=0.0026, time=278.36s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8acd42b6ede7474fa56403eada124562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=10] Epoch=02, train_loss=0.0002, val_loss=0.0027, time=280.13s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50db341d66a94e2d9f23b987ddabf7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=10] Epoch=03, train_loss=0.0002, val_loss=0.0027, time=274.83s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f701cf76c9e4ec68c55e859d180ae3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=10] Epoch=04, train_loss=0.0002, val_loss=0.0028, time=275.64s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3693ed49c001473288c20f560fc885cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=10] Epoch=05, train_loss=0.0002, val_loss=0.0026, time=282.73s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f215c785c8480ea1edaea80ad1397a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=10] Epoch=06, train_loss=0.0002, val_loss=0.0027, time=272.79s\n",
      "üïí [H=10] Average epoch time: 277.63 seconds over 7 epochs\n",
      "‚úÖ Fine-tuning complete for horizon 10. Saved to ckpts_lambda/horizon_10_lambda\n",
      "\n",
      "üîÅ Fine-tuning model for horizon = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0      | train\n",
      "3  | prescalers                         | ModuleDict                      | 3.8 K  | train\n",
      "4  | encoder_variable_selection         | VariableSelectionNetwork        | 819 K  | train\n",
      "5  | decoder_variable_selection         | VariableSelectionNetwork        | 749 K  | train\n",
      "6  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K | train\n",
      "7  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K | train\n",
      "8  | lstm_encoder                       | LSTM                            | 132 K  | train\n",
      "9  | lstm_decoder                       | LSTM                            | 132 K  | train\n",
      "10 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K | train\n",
      "11 | post_lstm_add_norm_encoder         | AddNorm                         | 256    | train\n",
      "12 | multihead_attn                     | InterpretableMultiHeadAttention | 41.2 K | train\n",
      "13 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K | train\n",
      "14 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K | train\n",
      "15 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K | train\n",
      "16 | output_layer                       | Linear                          | 387    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.687     Total estimated model params size (MB)\n",
      "357       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02c7e1e172445e29d05e75ca6fbabd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f116a2a7a864294ac94d4755bda1c74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4177693287a44019e210fe6d656c3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=15] Epoch=00, train_loss=0.0004, val_loss=0.0027, time=274.04s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a483e1acae65488687a10059eb9441c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=15] Epoch=01, train_loss=0.0003, val_loss=0.0028, time=275.68s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3910c8e932804d7496687ae574b416b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=15] Epoch=02, train_loss=0.0002, val_loss=0.0026, time=272.97s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b242ba5b69f04ebcb0661c3f2025680f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=15] Epoch=03, train_loss=0.0002, val_loss=0.0028, time=272.49s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0aca1965725479d9a8ec6b6be0767f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=15] Epoch=04, train_loss=0.0002, val_loss=0.0028, time=275.50s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0273f23e5a3f48508b28ae212b7b1d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=15] Epoch=05, train_loss=0.0002, val_loss=0.0028, time=283.23s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0274255c094a1991e23c6a816c9db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=15] Epoch=06, train_loss=0.0002, val_loss=0.0028, time=271.42s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d549140f24044132a86aaae7f578fca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=15] Epoch=07, train_loss=0.0002, val_loss=0.0028, time=275.84s\n",
      "üïí [H=15] Average epoch time: 275.15 seconds over 8 epochs\n",
      "‚úÖ Fine-tuning complete for horizon 15. Saved to ckpts_lambda/horizon_15_lambda\n",
      "\n",
      "üîÅ Fine-tuning model for horizon = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 0      | train\n",
      "3  | prescalers                         | ModuleDict                      | 3.8 K  | train\n",
      "4  | encoder_variable_selection         | VariableSelectionNetwork        | 819 K  | train\n",
      "5  | decoder_variable_selection         | VariableSelectionNetwork        | 749 K  | train\n",
      "6  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K | train\n",
      "7  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K | train\n",
      "8  | lstm_encoder                       | LSTM                            | 132 K  | train\n",
      "9  | lstm_decoder                       | LSTM                            | 132 K  | train\n",
      "10 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K | train\n",
      "11 | post_lstm_add_norm_encoder         | AddNorm                         | 256    | train\n",
      "12 | multihead_attn                     | InterpretableMultiHeadAttention | 41.2 K | train\n",
      "13 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K | train\n",
      "14 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K | train\n",
      "15 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K | train\n",
      "16 | output_layer                       | Linear                          | 387    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.687     Total estimated model params size (MB)\n",
      "357       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c739c02d11d34fd9a09512b4ebf651dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837ad82e65854ae69718a1d7d2432793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942e9bb00f3c40a1993fc12b3cd4dd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=20] Epoch=00, train_loss=0.0004, val_loss=0.0028, time=275.74s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32469231138424fb36df34e054db9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=20] Epoch=01, train_loss=0.0003, val_loss=0.0027, time=285.79s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d64b66c666c4d96bfb40d7607349df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=20] Epoch=02, train_loss=0.0002, val_loss=0.0027, time=278.35s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5652f0fa0ee54e44ba92c2b1ba34dbd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=20] Epoch=03, train_loss=0.0002, val_loss=0.0027, time=278.33s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f264f32f223945f9b31b638eeab3275f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=20] Epoch=04, train_loss=0.0002, val_loss=0.0027, time=278.73s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cdf4a86f08d488e98add142b1ceb328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=20] Epoch=05, train_loss=0.0002, val_loss=0.0028, time=276.95s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf28f60c1af4b0c847d5a3ad3057677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[H=20] Epoch=06, train_loss=0.0002, val_loss=0.0027, time=291.25s\n",
      "üïí [H=20] Average epoch time: 280.74 seconds over 7 epochs\n",
      "‚úÖ Fine-tuning complete for horizon 20. Saved to ckpts_lambda/horizon_20_lambda\n"
     ]
    }
   ],
   "source": [
    "horizons = [1,5, 10, 15, 20]\n",
    "for horizon in horizons:\n",
    "    print(f\"\\nüîÅ Fine-tuning model for horizon = {horizon}\")\n",
    "    ckpt_path = f\"checkpoints_no_static/horizon_{horizon}\"\n",
    "    best_ckpt_file = [f for f in os.listdir(ckpt_path) if f.endswith(\".ckpt\")]\n",
    "    assert len(best_ckpt_file) == 1, f\"Checkpoint missing in {ckpt_path}\"\n",
    "    best_ckpt = os.path.join(ckpt_path, best_ckpt_file[0])\n",
    "\n",
    "    training = TimeSeriesDataSet(\n",
    "        train_df,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=target,\n",
    "        group_ids=group_ids,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        max_prediction_length=horizon,\n",
    "        static_categoricals=static_categoricals,\n",
    "        static_reals=static_reals,\n",
    "        time_varying_known_reals=time_varying_known_reals,\n",
    "        time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "        target_normalizer=GroupNormalizer(groups=group_ids),\n",
    "        scalers=covariate_scalers,\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "    )\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, val_df, stop_randomization=True)\n",
    "\n",
    "    train_loader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "    val_loader = validation.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    # model = TemporalFusionTransformer.load_from_checkpoint(best_ckpt)\n",
    "    \n",
    "\n",
    "    ckpt_dir = f\"ckpts_lambda/horizon_{horizon}_lambda\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        dirpath=ckpt_dir,\n",
    "        filename=\"finetuned-{epoch:02d}-{val_loss:.4f}\",\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "    )\n",
    "    time_cb = TimeAndLossCallback(horizon)\n",
    "    model = TemporalFusionTransformer.from_dataset(training, **model_kwargs )\n",
    "    trainer = Trainer(callbacks=[early_stop, checkpoint, time_cb], **trainer_kwargs)\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    print(f\"‚úÖ Fine-tuning complete for horizon {horizon}. Saved to {ckpt_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d46ef9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluating fine-tuned model for horizon = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test Metrics (horizon=1):\n",
      "   MSE   = 0.003439\n",
      "   RMSE  = 0.058643\n",
      "   MAE   = 0.004061\n",
      "   MAPE  = 37.187183%\n",
      "   SMAPE = 14.897680%\n",
      "\n",
      "üìä Evaluating fine-tuned model for horizon = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test Metrics (horizon=5):\n",
      "   MSE   = 0.003477\n",
      "   RMSE  = 0.058965\n",
      "   MAE   = 0.004154\n",
      "   MAPE  = 40.143099%\n",
      "   SMAPE = 15.814334%\n",
      "\n",
      "üìä Evaluating fine-tuned model for horizon = 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test Metrics (horizon=10):\n",
      "   MSE   = 0.003492\n",
      "   RMSE  = 0.059091\n",
      "   MAE   = 0.004196\n",
      "   MAPE  = 33.164391%\n",
      "   SMAPE = 18.245224%\n",
      "\n",
      "üìä Evaluating fine-tuned model for horizon = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test Metrics (horizon=15):\n",
      "   MSE   = 0.003553\n",
      "   RMSE  = 0.059607\n",
      "   MAE   = 0.004204\n",
      "   MAPE  = 32.559642%\n",
      "   SMAPE = 16.431154%\n",
      "\n",
      "üìä Evaluating fine-tuned model for horizon = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test Metrics (horizon=20):\n",
      "   MSE   = 0.003497\n",
      "   RMSE  = 0.059138\n",
      "   MAE   = 0.004204\n",
      "   MAPE  = 30.333918%\n",
      "   SMAPE = 18.002275%\n"
     ]
    }
   ],
   "source": [
    "for horizon in horizons:\n",
    "    print(f\"\\nüìä Evaluating fine-tuned model for horizon = {horizon}\")\n",
    "\n",
    "    # Load original training dataset just for consistent normalization\n",
    "    train_df = df.iloc[:num_train]\n",
    "    training = TimeSeriesDataSet(\n",
    "        train_df,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=target,\n",
    "        group_ids=group_ids,\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        max_prediction_length=horizon,\n",
    "        static_categoricals=static_categoricals,\n",
    "        static_reals=static_reals,\n",
    "        time_varying_known_reals=time_varying_known_reals,\n",
    "        time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "        target_normalizer=GroupNormalizer(groups=group_ids),\n",
    "        scalers=covariate_scalers,\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=True,\n",
    "        add_encoder_length=True,\n",
    "    )\n",
    "\n",
    "    # Build test set using from_dataset\n",
    "    test = TimeSeriesDataSet.from_dataset(training, test_df, stop_randomization=True)\n",
    "    test_loader = test.to_dataloader(train=False, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    # Load fine-tuned model\n",
    "    ckpt_dir = f\"ckpts_lambda/horizon_{horizon}_lambda\"\n",
    "    ckpt_files = [f for f in os.listdir(ckpt_dir) if f.endswith(\".ckpt\")]\n",
    "    assert len(ckpt_files) == 1, f\"Expected one .ckpt file in {ckpt_dir}\"\n",
    "    best_ckpt = os.path.join(ckpt_dir, ckpt_files[0])\n",
    "    model = TemporalFusionTransformer.load_from_checkpoint(best_ckpt)\n",
    "\n",
    "    # Predict\n",
    "    preds = model.predict(test_loader, return_x=False).detach().cpu().numpy().flatten()\n",
    "\n",
    "    # True labels\n",
    "    actuals = np.concatenate([y.detach().cpu().numpy()\n",
    "                              for _, (y, _) in iter(test_loader)], axis=0).flatten()\n",
    "\n",
    "    # Metrics\n",
    "    mse = mean_squared_error(actuals, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, preds)\n",
    "    mape = mean_absolute_percentage_error(actuals, preds)\n",
    "    smape = 100 * np.mean(2 * np.abs(actuals - preds) / (np.abs(actuals) + np.abs(preds) + 1e-8))\n",
    "\n",
    "    print(f\"‚úÖ Test Metrics (horizon={horizon}):\")\n",
    "    print(f\"   MSE   = {mse:.6f}\")\n",
    "    print(f\"   RMSE  = {rmse:.6f}\")\n",
    "    print(f\"   MAE   = {mae:.6f}\")\n",
    "    print(f\"   MAPE  = {mape:.6%}\")\n",
    "    print(f\"   SMAPE = {smape:.6f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-forecasting-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
