
========================================================================================================================
RUN=1 | HORIZON=25 | ts=2026-01-19 21:10:54
========================================================================================================================
Training | run=1 horizon=25
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_1/horizon_25
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=1 H=25] Epoch=00 time(train+val)=171.105s
[run=1 H=25] Epoch=00 train_loss=0.000492 val_loss=0.000689
[run=1 H=25] Epoch=01 time(train+val)=169.994s
[run=1 H=25] Epoch=01 train_loss=0.000391 val_loss=0.000658
[run=1 H=25] Epoch=02 time(train+val)=166.163s
[run=1 H=25] Epoch=02 train_loss=0.000345 val_loss=0.000616
[run=1 H=25] Epoch=03 time(train+val)=164.736s
[run=1 H=25] Epoch=03 train_loss=0.000320 val_loss=0.000616
[run=1 H=25] Epoch=04 time(train+val)=163.445s
[run=1 H=25] Epoch=04 train_loss=0.000293 val_loss=0.000597
[run=1 H=25] Epoch=05 time(train+val)=163.054s
[run=1 H=25] Epoch=05 train_loss=0.000270 val_loss=0.000617
[run=1 H=25] Epoch=06 time(train+val)=162.544s
[run=1 H=25] Epoch=06 train_loss=0.000251 val_loss=0.000627
[run=1 H=25] Epoch=07 time(train+val)=163.041s
[run=1 H=25] Epoch=07 train_loss=0.000234 val_loss=0.000655
[run=1 H=25] Epoch=08 time(train+val)=161.891s
[run=1 H=25] Epoch=08 train_loss=0.000222 val_loss=0.000658
[run=1 H=25] Epoch=09 time(train+val)=163.127s
[run=1 H=25] Epoch=09 train_loss=0.000211 val_loss=0.000661

--- TRAINING TIME SUMMARY ---
epochs_ran=10
avg_epoch_time(train+val)=164.9099s
best_epoch=4
e2e_time_until_best=835.4424s
total_fit_time_until_stop=1661.3339s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_1/horizon_25/best-epoch=04-val_loss=0.0006.ckpt

Evaluating | run=1 horizon=25
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0008692021365277469, 'test_SMAPE': 0.07106916606426239, 'test_MAE': 0.0010579548543319106, 'test_RMSE': 0.00221951468847692, 'test_MAPE': 0.0662442296743393}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=25.3350s | per_batch=0.040797s
MSE=0.000034, RMSE=0.005837, MAE=0.001058, MAPE=6.624462%, SMAPE=7.106917%

END | run=1 horizon=25

========================================================================================================================
RUN=1 | HORIZON=30 | ts=2026-01-19 21:39:45
========================================================================================================================
Training | run=1 horizon=30
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_1/horizon_30
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=1 H=30] Epoch=00 time(train+val)=164.481s
[run=1 H=30] Epoch=00 train_loss=0.000489 val_loss=0.000677
[run=1 H=30] Epoch=01 time(train+val)=165.052s
[run=1 H=30] Epoch=01 train_loss=0.000370 val_loss=0.000613
[run=1 H=30] Epoch=02 time(train+val)=163.553s
[run=1 H=30] Epoch=02 train_loss=0.000345 val_loss=0.000628
[run=1 H=30] Epoch=03 time(train+val)=163.960s
[run=1 H=30] Epoch=03 train_loss=0.000323 val_loss=0.000634
[run=1 H=30] Epoch=04 time(train+val)=163.484s
[run=1 H=30] Epoch=04 train_loss=0.000299 val_loss=0.000627
[run=1 H=30] Epoch=05 time(train+val)=162.657s
[run=1 H=30] Epoch=05 train_loss=0.000273 val_loss=0.000628
[run=1 H=30] Epoch=06 time(train+val)=164.414s
[run=1 H=30] Epoch=06 train_loss=0.000252 val_loss=0.000625

--- TRAINING TIME SUMMARY ---
epochs_ran=7
avg_epoch_time(train+val)=163.9432s
best_epoch=1
e2e_time_until_best=329.5335s
total_fit_time_until_stop=1154.5743s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_1/horizon_30/best-epoch=01-val_loss=0.0006.ckpt

Evaluating | run=1 horizon=30
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0008581969887018204, 'test_SMAPE': 0.07559573650360107, 'test_MAE': 0.001048790174536407, 'test_RMSE': 0.0021715087350457907, 'test_MAPE': 0.07226557284593582}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=25.7714s | per_batch=0.041500s
MSE=0.000034, RMSE=0.005845, MAE=0.001049, MAPE=7.226642%, SMAPE=7.559568%

END | run=1 horizon=30

========================================================================================================================
RUN=1 | HORIZON=35 | ts=2026-01-19 22:00:09
========================================================================================================================
Training | run=1 horizon=35
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_1/horizon_35
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=1 H=35] Epoch=00 time(train+val)=164.410s
[run=1 H=35] Epoch=00 train_loss=0.000444 val_loss=0.000644
[run=1 H=35] Epoch=01 time(train+val)=166.301s
[run=1 H=35] Epoch=01 train_loss=0.000359 val_loss=0.000615
[run=1 H=35] Epoch=02 time(train+val)=165.114s
[run=1 H=35] Epoch=02 train_loss=0.000337 val_loss=0.000629
[run=1 H=35] Epoch=03 time(train+val)=166.501s
[run=1 H=35] Epoch=03 train_loss=0.000318 val_loss=0.000643
[run=1 H=35] Epoch=04 time(train+val)=164.547s
[run=1 H=35] Epoch=04 train_loss=0.000297 val_loss=0.000654
[run=1 H=35] Epoch=05 time(train+val)=165.072s
[run=1 H=35] Epoch=05 train_loss=0.000275 val_loss=0.000658
[run=1 H=35] Epoch=06 time(train+val)=164.266s
[run=1 H=35] Epoch=06 train_loss=0.000251 val_loss=0.000645

--- TRAINING TIME SUMMARY ---
epochs_ran=7
avg_epoch_time(train+val)=165.1732s
best_epoch=1
e2e_time_until_best=330.7116s
total_fit_time_until_stop=1163.0320s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_1/horizon_35/best-epoch=01-val_loss=0.0006.ckpt

Evaluating | run=1 horizon=35
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0008648417424410582, 'test_SMAPE': 0.07544571161270142, 'test_MAE': 0.0010497954208403826, 'test_RMSE': 0.0021963848266750574, 'test_MAPE': 0.07437117397785187}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=25.4067s | per_batch=0.040912s
MSE=0.000034, RMSE=0.005864, MAE=0.001050, MAPE=7.437234%, SMAPE=7.544574%

END | run=1 horizon=35

========================================================================================================================
RUN=1 | HORIZON=40 | ts=2026-01-19 22:20:41
========================================================================================================================
Training | run=1 horizon=40
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_1/horizon_40
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=1 H=40] Epoch=00 time(train+val)=167.413s
[run=1 H=40] Epoch=00 train_loss=0.000451 val_loss=0.000625
[run=1 H=40] Epoch=01 time(train+val)=166.383s
[run=1 H=40] Epoch=01 train_loss=0.000354 val_loss=0.000618
[run=1 H=40] Epoch=02 time(train+val)=165.878s
[run=1 H=40] Epoch=02 train_loss=0.000333 val_loss=0.000636
[run=1 H=40] Epoch=03 time(train+val)=167.497s
[run=1 H=40] Epoch=03 train_loss=0.000310 val_loss=0.000609
[run=1 H=40] Epoch=04 time(train+val)=165.352s
[run=1 H=40] Epoch=04 train_loss=0.000285 val_loss=0.000622
[run=1 H=40] Epoch=05 time(train+val)=167.395s
[run=1 H=40] Epoch=05 train_loss=0.000257 val_loss=0.000651
[run=1 H=40] Epoch=06 time(train+val)=167.150s
[run=1 H=40] Epoch=06 train_loss=0.000236 val_loss=0.000648
[run=1 H=40] Epoch=07 time(train+val)=167.358s
[run=1 H=40] Epoch=07 train_loss=0.000220 val_loss=0.000671
[run=1 H=40] Epoch=08 time(train+val)=166.577s
[run=1 H=40] Epoch=08 train_loss=0.000207 val_loss=0.000688

--- TRAINING TIME SUMMARY ---
epochs_ran=9
avg_epoch_time(train+val)=166.7783s
best_epoch=3
e2e_time_until_best=667.1716s
total_fit_time_until_stop=1510.3792s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_1/horizon_40/best-epoch=03-val_loss=0.0006.ckpt

Evaluating | run=1 horizon=40
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0007713228114880621, 'test_SMAPE': 0.07222610712051392, 'test_MAE': 0.0009781214175745845, 'test_RMSE': 0.002185054821893573, 'test_MAPE': 0.06823308020830154}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=26.2734s | per_batch=0.042308s
MSE=0.000031, RMSE=0.005535, MAE=0.000978, MAPE=6.823361%, SMAPE=7.222617%

END | run=1 horizon=40

========================================================================================================================
RUN=2 | HORIZON=25 | ts=2026-01-19 22:47:01
========================================================================================================================
Training | run=2 horizon=25
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_2/horizon_25
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=2 H=25] Epoch=00 time(train+val)=162.781s
[run=2 H=25] Epoch=00 train_loss=0.000451 val_loss=0.000622
[run=2 H=25] Epoch=01 time(train+val)=163.312s
[run=2 H=25] Epoch=01 train_loss=0.000359 val_loss=0.000613
[run=2 H=25] Epoch=02 time(train+val)=164.043s
[run=2 H=25] Epoch=02 train_loss=0.000342 val_loss=0.000609
[run=2 H=25] Epoch=03 time(train+val)=163.470s
[run=2 H=25] Epoch=03 train_loss=0.000329 val_loss=0.000586
[run=2 H=25] Epoch=04 time(train+val)=164.284s
[run=2 H=25] Epoch=04 train_loss=0.000307 val_loss=0.000594
[run=2 H=25] Epoch=05 time(train+val)=163.362s
[run=2 H=25] Epoch=05 train_loss=0.000288 val_loss=0.000605
[run=2 H=25] Epoch=06 time(train+val)=161.721s
[run=2 H=25] Epoch=06 train_loss=0.000270 val_loss=0.000624
[run=2 H=25] Epoch=07 time(train+val)=162.972s
[run=2 H=25] Epoch=07 train_loss=0.000251 val_loss=0.000658
[run=2 H=25] Epoch=08 time(train+val)=163.572s
[run=2 H=25] Epoch=08 train_loss=0.000234 val_loss=0.000708

--- TRAINING TIME SUMMARY ---
epochs_ran=9
avg_epoch_time(train+val)=163.2795s
best_epoch=3
e2e_time_until_best=653.6062s
total_fit_time_until_stop=1478.3327s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_2/horizon_25/best-epoch=03-val_loss=0.0006.ckpt

Evaluating | run=2 horizon=25
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0007806766079738736, 'test_SMAPE': 0.0695025697350502, 'test_MAE': 0.0009738747030496597, 'test_RMSE': 0.002072444185614586, 'test_MAPE': 0.06586994975805283}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=25.3168s | per_batch=0.040768s
MSE=0.000031, RMSE=0.005533, MAE=0.000974, MAPE=6.587039%, SMAPE=6.950259%

END | run=2 horizon=25

========================================================================================================================
RUN=2 | HORIZON=30 | ts=2026-01-19 23:12:49
========================================================================================================================
Training | run=2 horizon=30
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_2/horizon_30
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=2 H=30] Epoch=00 time(train+val)=164.973s
[run=2 H=30] Epoch=00 train_loss=0.000451 val_loss=0.000652
[run=2 H=30] Epoch=01 time(train+val)=164.612s
[run=2 H=30] Epoch=01 train_loss=0.000359 val_loss=0.000622
[run=2 H=30] Epoch=02 time(train+val)=163.401s
[run=2 H=30] Epoch=02 train_loss=0.000339 val_loss=0.000636
[run=2 H=30] Epoch=03 time(train+val)=163.456s
[run=2 H=30] Epoch=03 train_loss=0.000327 val_loss=0.000622
[run=2 H=30] Epoch=04 time(train+val)=162.593s
[run=2 H=30] Epoch=04 train_loss=0.000302 val_loss=0.000671
[run=2 H=30] Epoch=05 time(train+val)=164.007s
[run=2 H=30] Epoch=05 train_loss=0.000282 val_loss=0.000654
[run=2 H=30] Epoch=06 time(train+val)=162.575s
[run=2 H=30] Epoch=06 train_loss=0.000259 val_loss=0.000636

--- TRAINING TIME SUMMARY ---
epochs_ran=7
avg_epoch_time(train+val)=163.6595s
best_epoch=1
e2e_time_until_best=329.5842s
total_fit_time_until_stop=1152.5938s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_2/horizon_30/best-epoch=01-val_loss=0.0006.ckpt

Evaluating | run=2 horizon=30
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0008686989895068109, 'test_SMAPE': 0.07959160953760147, 'test_MAE': 0.0010647921590134501, 'test_RMSE': 0.0021982784382998943, 'test_MAPE': 0.07364533096551895}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=24.6855s | per_batch=0.039751s
MSE=0.000034, RMSE=0.005865, MAE=0.001065, MAPE=7.364599%, SMAPE=7.959159%

END | run=2 horizon=30

========================================================================================================================
RUN=2 | HORIZON=35 | ts=2026-01-19 23:33:09
========================================================================================================================
Training | run=2 horizon=35
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_2/horizon_35
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=2 H=35] Epoch=00 time(train+val)=163.584s
[run=2 H=35] Epoch=00 train_loss=0.000464 val_loss=0.000619
[run=2 H=35] Epoch=01 time(train+val)=165.159s
[run=2 H=35] Epoch=01 train_loss=0.000356 val_loss=0.000622
[run=2 H=35] Epoch=02 time(train+val)=165.037s
[run=2 H=35] Epoch=02 train_loss=0.000336 val_loss=0.000635
[run=2 H=35] Epoch=03 time(train+val)=166.898s
[run=2 H=35] Epoch=03 train_loss=0.000314 val_loss=0.000634
[run=2 H=35] Epoch=04 time(train+val)=165.824s
[run=2 H=35] Epoch=04 train_loss=0.000290 val_loss=0.000645
[run=2 H=35] Epoch=05 time(train+val)=165.922s
[run=2 H=35] Epoch=05 train_loss=0.000264 val_loss=0.000674

--- TRAINING TIME SUMMARY ---
epochs_ran=6
avg_epoch_time(train+val)=165.4039s
best_epoch=0
e2e_time_until_best=163.5840s
total_fit_time_until_stop=998.6638s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_2/horizon_35/best-epoch=00-val_loss=0.0006.ckpt

Evaluating | run=2 horizon=35
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0008751705172471702, 'test_SMAPE': 0.0900036096572876, 'test_MAE': 0.0010894745355471969, 'test_RMSE': 0.002241467824205756, 'test_MAPE': 0.10077361762523651}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=25.2720s | per_batch=0.040696s
MSE=0.000035, RMSE=0.005878, MAE=0.001089, MAPE=10.077608%, SMAPE=9.000359%

END | run=2 horizon=35

========================================================================================================================
RUN=2 | HORIZON=40 | ts=2026-01-19 23:50:56
========================================================================================================================
Training | run=2 horizon=40
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_2/horizon_40
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=2 H=40] Epoch=00 time(train+val)=166.164s
[run=2 H=40] Epoch=00 train_loss=0.000451 val_loss=0.000626
[run=2 H=40] Epoch=01 time(train+val)=166.571s
[run=2 H=40] Epoch=01 train_loss=0.000355 val_loss=0.000623
[run=2 H=40] Epoch=02 time(train+val)=167.611s
[run=2 H=40] Epoch=02 train_loss=0.000329 val_loss=0.000644
[run=2 H=40] Epoch=03 time(train+val)=164.720s
[run=2 H=40] Epoch=03 train_loss=0.000298 val_loss=0.000654
[run=2 H=40] Epoch=04 time(train+val)=165.749s
[run=2 H=40] Epoch=04 train_loss=0.000266 val_loss=0.000649
[run=2 H=40] Epoch=05 time(train+val)=164.696s
[run=2 H=40] Epoch=05 train_loss=0.000238 val_loss=0.000687
[run=2 H=40] Epoch=06 time(train+val)=167.017s
[run=2 H=40] Epoch=06 train_loss=0.000219 val_loss=0.000685

--- TRAINING TIME SUMMARY ---
epochs_ran=7
avg_epoch_time(train+val)=166.0755s
best_epoch=1
e2e_time_until_best=332.7350s
total_fit_time_until_stop=1169.7063s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_2/horizon_40/best-epoch=01-val_loss=0.0006.ckpt

Evaluating | run=2 horizon=40
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0008691957336850464, 'test_SMAPE': 0.0757288783788681, 'test_MAE': 0.0010591363534331322, 'test_RMSE': 0.0022351236548274755, 'test_MAPE': 0.07023297250270844}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=25.8022s | per_batch=0.041550s
MSE=0.000034, RMSE=0.005863, MAE=0.001059, MAPE=7.023353%, SMAPE=7.572889%

END | run=2 horizon=40

========================================================================================================================
RUN=3 | HORIZON=25 | ts=2026-01-20 00:11:34
========================================================================================================================
Training | run=3 horizon=25
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_3/horizon_25
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=3 H=25] Epoch=00 time(train+val)=162.665s
[run=3 H=25] Epoch=00 train_loss=0.000438 val_loss=0.000623
[run=3 H=25] Epoch=01 time(train+val)=161.385s
[run=3 H=25] Epoch=01 train_loss=0.000354 val_loss=0.000617
[run=3 H=25] Epoch=02 time(train+val)=163.074s
[run=3 H=25] Epoch=02 train_loss=0.000339 val_loss=0.000616
[run=3 H=25] Epoch=03 time(train+val)=160.926s
[run=3 H=25] Epoch=03 train_loss=0.000322 val_loss=0.000573
[run=3 H=25] Epoch=04 time(train+val)=161.986s
[run=3 H=25] Epoch=04 train_loss=0.000307 val_loss=0.000572
[run=3 H=25] Epoch=05 time(train+val)=160.971s
[run=3 H=25] Epoch=05 train_loss=0.000286 val_loss=0.000596
[run=3 H=25] Epoch=06 time(train+val)=161.761s
[run=3 H=25] Epoch=06 train_loss=0.000270 val_loss=0.000623
[run=3 H=25] Epoch=07 time(train+val)=161.082s
[run=3 H=25] Epoch=07 train_loss=0.000249 val_loss=0.000630
[run=3 H=25] Epoch=08 time(train+val)=162.850s
[run=3 H=25] Epoch=08 train_loss=0.000234 val_loss=0.000627
[run=3 H=25] Epoch=09 time(train+val)=160.738s
[run=3 H=25] Epoch=09 train_loss=0.000223 val_loss=0.000616

--- TRAINING TIME SUMMARY ---
epochs_ran=10
avg_epoch_time(train+val)=161.7439s
best_epoch=4
e2e_time_until_best=810.0357s
total_fit_time_until_stop=1627.8076s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_3/horizon_25/best-epoch=04-val_loss=0.0006.ckpt

Evaluating | run=3 horizon=25
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0007618594099767506, 'test_SMAPE': 0.06501505523920059, 'test_MAE': 0.0009629502892494202, 'test_RMSE': 0.0021068367641419172, 'test_MAPE': 0.06262326240539551}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=24.0824s | per_batch=0.038780s
MSE=0.000030, RMSE=0.005483, MAE=0.000963, MAPE=6.262358%, SMAPE=6.501505%

END | run=3 horizon=25

========================================================================================================================
RUN=3 | HORIZON=30 | ts=2026-01-20 00:39:49
========================================================================================================================
Training | run=3 horizon=30
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_3/horizon_30
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=3 H=30] Epoch=00 time(train+val)=161.917s
[run=3 H=30] Epoch=00 train_loss=0.000445 val_loss=0.000644
[run=3 H=30] Epoch=01 time(train+val)=160.911s
[run=3 H=30] Epoch=01 train_loss=0.000357 val_loss=0.000621
[run=3 H=30] Epoch=02 time(train+val)=160.933s
[run=3 H=30] Epoch=02 train_loss=0.000339 val_loss=0.000626
[run=3 H=30] Epoch=03 time(train+val)=161.498s
[run=3 H=30] Epoch=03 train_loss=0.000320 val_loss=0.000640
[run=3 H=30] Epoch=04 time(train+val)=160.392s
[run=3 H=30] Epoch=04 train_loss=0.000300 val_loss=0.000626
[run=3 H=30] Epoch=05 time(train+val)=160.780s
[run=3 H=30] Epoch=05 train_loss=0.000276 val_loss=0.000645
[run=3 H=30] Epoch=06 time(train+val)=160.697s
[run=3 H=30] Epoch=06 train_loss=0.000256 val_loss=0.000641

--- TRAINING TIME SUMMARY ---
epochs_ran=7
avg_epoch_time(train+val)=161.0184s
best_epoch=1
e2e_time_until_best=322.8286s
total_fit_time_until_stop=1133.9196s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_3/horizon_30/best-epoch=01-val_loss=0.0006.ckpt

Evaluating | run=3 horizon=30
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0008619343861937523, 'test_SMAPE': 0.07264849543571472, 'test_MAE': 0.001047078869305551, 'test_RMSE': 0.002181742340326309, 'test_MAPE': 0.06656573712825775}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=24.6003s | per_batch=0.039614s
MSE=0.000035, RMSE=0.005883, MAE=0.001047, MAPE=6.656615%, SMAPE=7.264847%

END | run=3 horizon=30

========================================================================================================================
RUN=3 | HORIZON=35 | ts=2026-01-20 00:59:50
========================================================================================================================
Training | run=3 horizon=35
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_3/horizon_35
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=3 H=35] Epoch=00 time(train+val)=164.490s
[run=3 H=35] Epoch=00 train_loss=0.000444 val_loss=0.000624
[run=3 H=35] Epoch=01 time(train+val)=162.308s
[run=3 H=35] Epoch=01 train_loss=0.000356 val_loss=0.000623
[run=3 H=35] Epoch=02 time(train+val)=162.607s
[run=3 H=35] Epoch=02 train_loss=0.000336 val_loss=0.000634
[run=3 H=35] Epoch=03 time(train+val)=161.852s
[run=3 H=35] Epoch=03 train_loss=0.000314 val_loss=0.000597
[run=3 H=35] Epoch=04 time(train+val)=162.764s
[run=3 H=35] Epoch=04 train_loss=0.000289 val_loss=0.000623
[run=3 H=35] Epoch=05 time(train+val)=163.157s
[run=3 H=35] Epoch=05 train_loss=0.000259 val_loss=0.000632
[run=3 H=35] Epoch=06 time(train+val)=164.085s
[run=3 H=35] Epoch=06 train_loss=0.000240 val_loss=0.000660
[run=3 H=35] Epoch=07 time(train+val)=162.911s
[run=3 H=35] Epoch=07 train_loss=0.000220 val_loss=0.000652
[run=3 H=35] Epoch=08 time(train+val)=164.245s
[run=3 H=35] Epoch=08 train_loss=0.000209 val_loss=0.000679

--- TRAINING TIME SUMMARY ---
epochs_ran=9
avg_epoch_time(train+val)=163.1578s
best_epoch=3
e2e_time_until_best=651.2563s
total_fit_time_until_stop=1477.2126s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_3/horizon_35/best-epoch=03-val_loss=0.0006.ckpt

Evaluating | run=3 horizon=35
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0008527095196768641, 'test_SMAPE': 0.07082440704107285, 'test_MAE': 0.0010375225683674216, 'test_RMSE': 0.0022032398264855146, 'test_MAPE': 0.06569626182317734}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=24.8353s | per_batch=0.039992s
MSE=0.000034, RMSE=0.005798, MAE=0.001038, MAPE=6.569665%, SMAPE=7.082438%

END | run=3 horizon=35

========================================================================================================================
RUN=3 | HORIZON=40 | ts=2026-01-20 01:25:34
========================================================================================================================
Training | run=3 horizon=40
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_3/horizon_40
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=3 H=40] Epoch=00 time(train+val)=165.030s
[run=3 H=40] Epoch=00 train_loss=0.000440 val_loss=0.000627
[run=3 H=40] Epoch=01 time(train+val)=165.858s
[run=3 H=40] Epoch=01 train_loss=0.000354 val_loss=0.000619
[run=3 H=40] Epoch=02 time(train+val)=166.321s
[run=3 H=40] Epoch=02 train_loss=0.000334 val_loss=0.000636
[run=3 H=40] Epoch=03 time(train+val)=164.058s
[run=3 H=40] Epoch=03 train_loss=0.000312 val_loss=0.000656
[run=3 H=40] Epoch=04 time(train+val)=166.166s
[run=3 H=40] Epoch=04 train_loss=0.000288 val_loss=0.000627
[run=3 H=40] Epoch=05 time(train+val)=167.553s
[run=3 H=40] Epoch=05 train_loss=0.000262 val_loss=0.000641
[run=3 H=40] Epoch=06 time(train+val)=165.290s
[run=3 H=40] Epoch=06 train_loss=0.000239 val_loss=0.000665

--- TRAINING TIME SUMMARY ---
epochs_ran=7
avg_epoch_time(train+val)=165.7538s
best_epoch=1
e2e_time_until_best=330.8883s
total_fit_time_until_stop=1166.7959s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_3/horizon_40/best-epoch=01-val_loss=0.0006.ckpt

Evaluating | run=3 horizon=40
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0008691866532899439, 'test_SMAPE': 0.07921231538057327, 'test_MAE': 0.0010599615052342415, 'test_RMSE': 0.0022308360785245895, 'test_MAPE': 0.0815972313284874}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=25.3856s | per_batch=0.040879s
MSE=0.000034, RMSE=0.005870, MAE=0.001060, MAPE=8.159892%, SMAPE=7.921234%

END | run=3 horizon=40

========================================================================================================================
RUN=4 | HORIZON=25 | ts=2026-01-20 01:46:10
========================================================================================================================
Training | run=4 horizon=25
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_4/horizon_25
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=4 H=25] Epoch=00 time(train+val)=161.194s
[run=4 H=25] Epoch=00 train_loss=0.000460 val_loss=0.000658
[run=4 H=25] Epoch=01 time(train+val)=161.008s
[run=4 H=25] Epoch=01 train_loss=0.000357 val_loss=0.000604
[run=4 H=25] Epoch=02 time(train+val)=161.609s
[run=4 H=25] Epoch=02 train_loss=0.000340 val_loss=0.000633
[run=4 H=25] Epoch=03 time(train+val)=161.007s
[run=4 H=25] Epoch=03 train_loss=0.000322 val_loss=0.000570
[run=4 H=25] Epoch=04 time(train+val)=162.086s
[run=4 H=25] Epoch=04 train_loss=0.000306 val_loss=0.000574
[run=4 H=25] Epoch=05 time(train+val)=162.278s
[run=4 H=25] Epoch=05 train_loss=0.000288 val_loss=0.000590
[run=4 H=25] Epoch=06 time(train+val)=162.900s
[run=4 H=25] Epoch=06 train_loss=0.000272 val_loss=0.000600
[run=4 H=25] Epoch=07 time(train+val)=162.508s
[run=4 H=25] Epoch=07 train_loss=0.000254 val_loss=0.000612
[run=4 H=25] Epoch=08 time(train+val)=161.319s
[run=4 H=25] Epoch=08 train_loss=0.000239 val_loss=0.000634

--- TRAINING TIME SUMMARY ---
epochs_ran=9
avg_epoch_time(train+val)=161.7678s
best_epoch=3
e2e_time_until_best=644.8191s
total_fit_time_until_stop=1464.5424s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_4/horizon_25/best-epoch=03-val_loss=0.0006.ckpt

Evaluating | run=4 horizon=25
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0007473045843653381, 'test_SMAPE': 0.07342314720153809, 'test_MAE': 0.0009508213261142373, 'test_RMSE': 0.0020570673514157534, 'test_MAPE': 0.08002365380525589}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=24.0067s | per_batch=0.038658s
MSE=0.000030, RMSE=0.005446, MAE=0.000951, MAPE=8.002524%, SMAPE=7.342318%

END | run=4 horizon=25

========================================================================================================================
RUN=4 | HORIZON=30 | ts=2026-01-20 02:11:41
========================================================================================================================
Training | run=4 horizon=30
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_4/horizon_30
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=4 H=30] Epoch=00 time(train+val)=163.328s
[run=4 H=30] Epoch=00 train_loss=0.000449 val_loss=0.000626
[run=4 H=30] Epoch=01 time(train+val)=161.695s
[run=4 H=30] Epoch=01 train_loss=0.000355 val_loss=0.000620
[run=4 H=30] Epoch=02 time(train+val)=164.458s
[run=4 H=30] Epoch=02 train_loss=0.000335 val_loss=0.000618
[run=4 H=30] Epoch=03 time(train+val)=163.997s
[run=4 H=30] Epoch=03 train_loss=0.000315 val_loss=0.000609
[run=4 H=30] Epoch=04 time(train+val)=164.099s
[run=4 H=30] Epoch=04 train_loss=0.000293 val_loss=0.000592
[run=4 H=30] Epoch=05 time(train+val)=163.158s
[run=4 H=30] Epoch=05 train_loss=0.000271 val_loss=0.000615
[run=4 H=30] Epoch=06 time(train+val)=162.386s
[run=4 H=30] Epoch=06 train_loss=0.000247 val_loss=0.000658
[run=4 H=30] Epoch=07 time(train+val)=162.850s
[run=4 H=30] Epoch=07 train_loss=0.000234 val_loss=0.000640
[run=4 H=30] Epoch=08 time(train+val)=164.466s
[run=4 H=30] Epoch=08 train_loss=0.000219 val_loss=0.000656
[run=4 H=30] Epoch=09 time(train+val)=164.144s
[run=4 H=30] Epoch=09 train_loss=0.000206 val_loss=0.000689

--- TRAINING TIME SUMMARY ---
epochs_ran=10
avg_epoch_time(train+val)=163.4580s
best_epoch=4
e2e_time_until_best=817.5770s
total_fit_time_until_stop=1644.4834s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_4/horizon_30/best-epoch=04-val_loss=0.0006.ckpt

Evaluating | run=4 horizon=30
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0007935453322716057, 'test_SMAPE': 0.07039456069469452, 'test_MAE': 0.000986715778708458, 'test_RMSE': 0.002162650693207979, 'test_MAPE': 0.07043763250112534}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=25.5875s | per_batch=0.041204s
MSE=0.000031, RMSE=0.005531, MAE=0.000987, MAPE=7.043853%, SMAPE=7.039452%

END | run=4 horizon=30

========================================================================================================================
RUN=4 | HORIZON=35 | ts=2026-01-20 02:40:15
========================================================================================================================
Training | run=4 horizon=35
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_4/horizon_35
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=4 H=35] Epoch=00 time(train+val)=162.938s
[run=4 H=35] Epoch=00 train_loss=0.000465 val_loss=0.000622
[run=4 H=35] Epoch=01 time(train+val)=162.551s
[run=4 H=35] Epoch=01 train_loss=0.000359 val_loss=0.000634
[run=4 H=35] Epoch=02 time(train+val)=162.168s
[run=4 H=35] Epoch=02 train_loss=0.000338 val_loss=0.000634
[run=4 H=35] Epoch=03 time(train+val)=164.465s
[run=4 H=35] Epoch=03 train_loss=0.000316 val_loss=0.000610
[run=4 H=35] Epoch=04 time(train+val)=162.867s
[run=4 H=35] Epoch=04 train_loss=0.000289 val_loss=0.000651
[run=4 H=35] Epoch=05 time(train+val)=163.329s
[run=4 H=35] Epoch=05 train_loss=0.000263 val_loss=0.000655
[run=4 H=35] Epoch=06 time(train+val)=164.250s
[run=4 H=35] Epoch=06 train_loss=0.000242 val_loss=0.000665
[run=4 H=35] Epoch=07 time(train+val)=164.025s
[run=4 H=35] Epoch=07 train_loss=0.000227 val_loss=0.000654
[run=4 H=35] Epoch=08 time(train+val)=163.279s
[run=4 H=35] Epoch=08 train_loss=0.000214 val_loss=0.000669

--- TRAINING TIME SUMMARY ---
epochs_ran=9
avg_epoch_time(train+val)=163.3192s
best_epoch=3
e2e_time_until_best=652.1226s
total_fit_time_until_stop=1478.7242s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_4/horizon_35/best-epoch=03-val_loss=0.0006.ckpt

Evaluating | run=4 horizon=35
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0008193799876607955, 'test_SMAPE': 0.07101930677890778, 'test_MAE': 0.0010182589758187532, 'test_RMSE': 0.0022122126538306475, 'test_MAPE': 0.06938531994819641}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=26.0212s | per_batch=0.041902s
MSE=0.000032, RMSE=0.005661, MAE=0.001018, MAPE=6.938602%, SMAPE=7.101934%

END | run=4 horizon=35

========================================================================================================================
RUN=4 | HORIZON=40 | ts=2026-01-20 03:06:04
========================================================================================================================
Training | run=4 horizon=40
Checkpoint dir: ckpts_no_static_multirun_metrics_lon/run_4/horizon_40
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
[run=4 H=40] Epoch=00 time(train+val)=167.268s
[run=4 H=40] Epoch=00 train_loss=0.000470 val_loss=0.000633
[run=4 H=40] Epoch=01 time(train+val)=165.833s
[run=4 H=40] Epoch=01 train_loss=0.000358 val_loss=0.000624
[run=4 H=40] Epoch=02 time(train+val)=164.987s
[run=4 H=40] Epoch=02 train_loss=0.000336 val_loss=0.000640
[run=4 H=40] Epoch=03 time(train+val)=166.266s
[run=4 H=40] Epoch=03 train_loss=0.000309 val_loss=0.000638
[run=4 H=40] Epoch=04 time(train+val)=165.152s
[run=4 H=40] Epoch=04 train_loss=0.000281 val_loss=0.000673
[run=4 H=40] Epoch=05 time(train+val)=165.687s
[run=4 H=40] Epoch=05 train_loss=0.000254 val_loss=0.000652
[run=4 H=40] Epoch=06 time(train+val)=165.172s
[run=4 H=40] Epoch=06 train_loss=0.000234 val_loss=0.000656

--- TRAINING TIME SUMMARY ---
epochs_ran=7
avg_epoch_time(train+val)=165.7664s
best_epoch=1
e2e_time_until_best=333.1011s
total_fit_time_until_stop=1167.5448s
best_ckpt=/home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_4/horizon_40/best-epoch=01-val_loss=0.0006.ckpt

Evaluating | run=4 horizon=40
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.
trainer.test: [{'test_loss': 0.0008708043023943901, 'test_SMAPE': 0.07476849108934402, 'test_MAE': 0.0010677904356271029, 'test_RMSE': 0.0022544837556779385, 'test_MAPE': 0.0715508759021759}]
/home/unt.ad.unt.edu/srr0248/.conda/envs/pytorch-forecasting-dev/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.

--- INFERENCE + METRICS ---
inference_time_total=25.4169s | per_batch=0.040929s
MSE=0.000035, RMSE=0.005876, MAE=0.001068, MAPE=7.155161%, SMAPE=7.476845%

END | run=4 horizon=40

########################################################################################################################
FINAL SUMMARY (per run)
########################################################################################################################
 run  horizon  avg_epoch_time_s  e2e_until_best_s  total_fit_time_s  best_epoch                                                                                                                                                              best_ckpt  inference_time_s      mse     rmse      mae     mape    smape
   1       25        164.909908        835.442388       1661.333887           4 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_1/horizon_25/best-epoch=04-val_loss=0.0006.ckpt         25.335014 0.000034 0.005837 0.001058 0.066245 7.106917
   2       25        163.279523        653.606180       1478.332695           3 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_2/horizon_25/best-epoch=03-val_loss=0.0006.ckpt         25.316782 0.000031 0.005533 0.000974 0.065870 6.950259
   3       25        161.743866        810.035681       1627.807592           4 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_3/horizon_25/best-epoch=04-val_loss=0.0006.ckpt         24.082360 0.000030 0.005483 0.000963 0.062624 6.501505
   4       25        161.767772        644.819130       1464.542429           3 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_4/horizon_25/best-epoch=03-val_loss=0.0006.ckpt         24.006727 0.000030 0.005446 0.000951 0.080025 7.342318
   1       30        163.943154        329.533467       1154.574267           1 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_1/horizon_30/best-epoch=01-val_loss=0.0006.ckpt         25.771410 0.000034 0.005845 0.001049 0.072266 7.559568
   2       30        163.659517        329.584173       1152.593754           1 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_2/horizon_30/best-epoch=01-val_loss=0.0006.ckpt         24.685489 0.000034 0.005865 0.001065 0.073646 7.959159
   3       30        161.018389        322.828618       1133.919628           1 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_3/horizon_30/best-epoch=01-val_loss=0.0006.ckpt         24.600271 0.000035 0.005883 0.001047 0.066566 7.264847
   4       30        163.458031        817.576997       1644.483354           4 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_4/horizon_30/best-epoch=04-val_loss=0.0006.ckpt         25.587457 0.000031 0.005531 0.000987 0.070439 7.039452
   1       35        165.173168        330.711555       1163.032027           1 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_1/horizon_35/best-epoch=01-val_loss=0.0006.ckpt         25.406654 0.000034 0.005864 0.001050 0.074372 7.544574
   2       35        165.403933        163.584020        998.663755           0 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_2/horizon_35/best-epoch=00-val_loss=0.0006.ckpt         25.271994 0.000035 0.005878 0.001089 0.100776 9.000359
   3       35        163.157758        651.256259       1477.212587           3 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_3/horizon_35/best-epoch=03-val_loss=0.0006.ckpt         24.835276 0.000034 0.005798 0.001038 0.065697 7.082438
   4       35        163.319183        652.122591       1478.724222           3 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_4/horizon_35/best-epoch=03-val_loss=0.0006.ckpt         26.021155 0.000032 0.005661 0.001018 0.069386 7.101934
   1       40        166.778277        667.171601       1510.379189           3 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_1/horizon_40/best-epoch=03-val_loss=0.0006.ckpt         26.273372 0.000031 0.005535 0.000978 0.068234 7.222617
   2       40        166.075482        332.735049       1169.706266           1 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_2/horizon_40/best-epoch=01-val_loss=0.0006.ckpt         25.802245 0.000034 0.005863 0.001059 0.070234 7.572889
   3       40        165.753767        330.888349       1166.795881           1 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_3/horizon_40/best-epoch=01-val_loss=0.0006.ckpt         25.385596 0.000034 0.005870 0.001060 0.081599 7.921234
   4       40        165.766409        333.101064       1167.544846           1 /home/unt.ad.unt.edu/srr0248/Desktop/pytorch_for/pytorch-forecasting/examples/ckpts_no_static_multirun_metrics_lon/run_4/horizon_40/best-epoch=01-val_loss=0.0006.ckpt         25.416888 0.000035 0.005876 0.001068 0.071552 7.476845

########################################################################################################################
FINAL SUMMARY (mean/std by horizon across runs)
########################################################################################################################
 horizon  avg_epoch_time_s_mean  avg_epoch_time_s_std  e2e_until_best_s_mean  e2e_until_best_s_std  inference_time_s_mean  inference_time_s_std  mse_mean  mse_std  smape_mean  smape_std
      25             162.925267              1.505523             735.975845            100.784855              24.685221              0.740472  0.000031 0.000002    6.975250   0.354558
      30             163.019773              1.349014             449.880814            245.151320              25.161157              0.604150  0.000033 0.000002    7.455756   0.397470
      35             164.263510              1.189185             449.418606            243.324282              25.383770              0.489939  0.000034 0.000001    7.682326   0.904232
      40             166.093484              0.480156             415.974016            167.467857              25.719525              0.415005  0.000033 0.000002    7.548396   0.289171
